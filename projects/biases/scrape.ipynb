{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38a5eab7-9c6c-48fb-93ba-dd68d374816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "#!pip install mediawikiapi\n",
    "# from mediawikiapi import MediaWikiAPI\n",
    "# wikipedia = MediaWikiAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bd220a0-695a-4f25-869d-2eb5ac4a2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slugify(value, allow_unicode=False):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n",
    "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
    "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
    "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
    "    trailing whitespace, dashes, and underscores.\n",
    "    \"\"\"\n",
    "    value = str(value)\n",
    "    if allow_unicode:\n",
    "        value = unicodedata.normalize('NFKC', value)\n",
    "    else:\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
    "    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n",
    "\n",
    "\n",
    "class WikiSearch:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.already_acquired = set()\n",
    "\n",
    "    def get_content(self,search_criteria,name):\n",
    "        \"\"\"Get content based on specific search criteria.\n",
    "        \n",
    "        Args:\n",
    "            search_criteria (str): search string from wikipedia suggestion\n",
    "            name (str): search string based on config\n",
    "        Return:\n",
    "            content (dict): payload containing content from API call\n",
    "                {\n",
    "                    links:list,\n",
    "                    references:list,\n",
    "                    raw_html:str,\n",
    "                    see_also:list,\n",
    "                    name\"str\n",
    "                }\n",
    "        \"\"\"\n",
    "        output = {}\n",
    "        results = None\n",
    "        try:\n",
    "            results = wikipedia.page(search_criteria,auto_suggest=False)\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            print(\"tried for {}, did not work so trying {}\".format(search_criteria,name))\n",
    "            try:\n",
    "                results = wikipedia.page(name,auto_suggest=False)\n",
    "            except wikipedia.exceptions.DisambiguationError as e:\n",
    "                print(\"did not work for {}\".format(name))\n",
    "                return None\n",
    "\n",
    "        position = None\n",
    "        if [x for x in results.content.split(\"\\n\") if \"== See\" in x]:\n",
    "            for idx, res in enumerate(results.content.split(\"\\n\")):\n",
    "                if \"== See\" in res:\n",
    "                    position = idx\n",
    "                    break\n",
    "\n",
    "        see_also = []\n",
    "        if position:\n",
    "            for content in results.content.split(\"\\n\")[position+1:]:\n",
    "                if content:\n",
    "                    if \"==\" in content:\n",
    "                        break\n",
    "                    see_also.append(content)\n",
    "\n",
    "        output[\"see_also\"] = see_also\n",
    "        output[\"name\"] = name\n",
    "        output[\"links\"] = results.links\n",
    "        output[\"raw_html\"] = results.html()\n",
    "        try:# Weird issue with the wrapper\n",
    "            output[\"references\"] = results.references\n",
    "        except KeyError:\n",
    "            output[\"references\"] = []\n",
    "        \n",
    "        self.already_acquired.add(search_criteria)\n",
    "        self.already_acquired.add(name)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def acquire_information(self,search_criteria):\n",
    "        \"\"\"Extract information from the API.\n",
    "        Entry point into main acquisition.\n",
    "        \n",
    "        Args:\n",
    "            search_criteria (str): name from config\n",
    "        Return:\n",
    "            content (dict): content from get_content()\n",
    "        \"\"\"\n",
    "        print(\"Acquiring information for {}\".format(search_criteria))\n",
    "        if \"(disambiguation)\" in search_criteria:\n",
    "            print(\"ignoring: {}\".format(search_criteria))\n",
    "            return {\n",
    "                \"references\":[],\n",
    "                \"links\":[],\n",
    "                \"raw_html\":\"\",\n",
    "                \"see_also\":[],\n",
    "                \"name\":search_criteria,\n",
    "                \"no_data\":True\n",
    "            }\n",
    "\n",
    "        check = wikipedia.search(search_criteria,results=1,suggestion=True)\n",
    "        check = check[0][0] if isinstance(check[0],list) else check[0]\n",
    "        \n",
    "        if (check not in self.already_acquired) or (search_criteria not in self.already_acquired):\n",
    "            output = self.get_content(check,search_criteria)\n",
    "            if not output:\n",
    "                print(\"no data for {}\".format(check))\n",
    "                return {\n",
    "                    \"references\":[],\n",
    "                    \"links\":[],\n",
    "                    \"raw_html\":\"\",\n",
    "                    \"see_also\":[],\n",
    "                    \"name\":search_criteria,\n",
    "                    \"no_data\":True\n",
    "                }\n",
    "            return output\n",
    "        else:\n",
    "            print(\"Already Acquired {}\".format(check))\n",
    "            return {\n",
    "                \"references\":[],\n",
    "                \"links\":[],\n",
    "                \"raw_html\":\"\",\n",
    "                \"see_also\":[],\n",
    "                \"name\":search_criteria,\n",
    "                \"no_data\":False,\n",
    "                \"already_acquired\":True\n",
    "            }\n",
    "\n",
    "\n",
    "    def get_nested_content(self,output):\n",
    "        \"\"\"Get all of the data for dependent links\"\"\"\n",
    "        print(\"getting nested content for: {}\".format(output.get(\"name\")))\n",
    "\n",
    "        related_content = {}\n",
    "        related_content[\"links\"] = []\n",
    "        related_content[\"see_also\"] = []\n",
    "\n",
    "        if output.get(\"no_data\",False):\n",
    "            print(\"no data for {}\".format(output.get(\"name\")))\n",
    "            return related_content\n",
    "        if output.get(\"already_acquired\",False):\n",
    "            print(\" Already got data for {}\".format(output.get(\"name\")))\n",
    "            return related_content\n",
    "\n",
    "        if output.get(\"links\"):\n",
    "            for link in output.get(\"links\"):\n",
    "                related_content[\"links\"].append(self.acquire_information(link))\n",
    "\n",
    "        if output.get(\"see_also\"):\n",
    "            for also in output.get(\"see_also\"):\n",
    "                related_content[\"see_also\"].append(self.acquire_information(link))\n",
    "\n",
    "        return related_content\n",
    "\n",
    "    def write_out(\n",
    "        self,\n",
    "        path,\n",
    "        content,\n",
    "        file_type\n",
    "    ):\n",
    "        if file_type == \"html\":\n",
    "            with open(path,\"w\",encoding=\"utf-8\") as out:\n",
    "                out.write(content)\n",
    "        elif file_type == \"txt\":\n",
    "            if not content:\n",
    "                return\n",
    "            with open(path,\"w\") as out:\n",
    "                out.write(\"\\n\".join(str(item) for item in content)) \n",
    "\n",
    "        return True\n",
    "\n",
    "    def write_to_path(\n",
    "        self,\n",
    "        path,\n",
    "        content\n",
    "    ):\n",
    "\n",
    "        if not os.path.exists(os.getcwd() + \"\\content\\{}\".format(path)):\n",
    "            os.makedirs(os.getcwd() + \"\\content\\{}\".format(path))\n",
    "\n",
    "        self.write_out(\n",
    "            os.getcwd() + \"\\content\\{}\\{}.html\".format(\n",
    "                path,\n",
    "                slugify(content.get(\"name\"))\n",
    "            ),\n",
    "            content.get(\"raw_html\"),\n",
    "            file_type = \"html\"\n",
    "        )\n",
    "\n",
    "        self.write_out(\n",
    "            os.getcwd() + \"\\content\\{}\\{}.txt\".format(\n",
    "                path,\n",
    "                \"sources\"\n",
    "            ),\n",
    "            content.get(\"references\"),\n",
    "            file_type = \"txt\"\n",
    "        )\n",
    "\n",
    "    def iterate_through_results(self,name_of_concept,depth=0):\n",
    "        \"\"\"Main function that iterates through the nested tree structure of data.\n",
    "        \n",
    "        Args:\n",
    "            name_of_concept (str): name from config\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        depth = depth\n",
    "        content = self.acquire_information(name_of_concept)\n",
    "        self.write_to_path(\n",
    "            path = slugify(content.get(\"name\")),\n",
    "            content = content\n",
    "        )\n",
    "\n",
    "        nested_content = self.get_nested_content(content)\n",
    "        first_level = []\n",
    "        first_level+=nested_content[\"links\"]\n",
    "        first_level+=nested_content[\"see_also\"]\n",
    "        new_content = first_level\n",
    "        while depth >= 0:\n",
    "\n",
    "            for stuff in new_content:\n",
    "                self.write_to_path(\n",
    "                    path = slugify(content.get(\"name\")) + \"\\\\\" + slugify(stuff.get(\"name\")),\n",
    "                    content = stuff\n",
    "                )\n",
    "            \n",
    "            if depth == 0:\n",
    "                break\n",
    "\n",
    "            new_content_temp = []\n",
    "            for stuff in new_content:\n",
    "                print(\"nested info: {}\".format(stuff.get(\"name\")))\n",
    "                #output = self.acquire_information(stuff.get(\"name\"))\n",
    "                nested_content = self.get_nested_content(stuff)\n",
    "                new_content_temp+=nested_content[\"links\"]\n",
    "                new_content_temp+=nested_content[\"see_also\"]\n",
    "\n",
    "            new_content = new_content_temp\n",
    "            print(len(new_content))\n",
    "\n",
    "            depth-=1\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6fa64bb0-df6b-4832-8a56-c5049493cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info():\n",
    "    search = WikiSearch()\n",
    "    with open(\"search_data.txt\",\"r\") as f:\n",
    "        for line in f:\n",
    "            search_criteria = line.strip()\n",
    "            if (\"Division\" in search_criteria) or (\"Composition\" in search_criteria):\n",
    "                search_criteria = \"Fallacy of \" + search_criteria\n",
    "            search.iterate_through_results(search_criteria)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36068571-2aab-4e43-b5b7-294f99010ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ffbf62-62c9-474e-b762-b039f565e3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
